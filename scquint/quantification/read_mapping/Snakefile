from collections import Counter, defaultdict

import numpy as np
import pandas as pd


fastq_paths = pd.read_csv(config['fastq_paths'], "\t", header=None, index_col=0, names=["fastq_1", "fastq_2"])

#fastq_paths = fastq_paths.sample(frac=1, random_state=43)

sample_ids = fastq_paths.index.values
print(sample_ids.shape)
#sample_ids = sample_ids[int(config["turn"])::int(config["every"])]
#print(sample_ids.shape)
#print(sample_ids)

n_splits = int(np.cbrt(len(sample_ids)))
splits = np.arange(n_splits)

min_overhang_unannotated_canonical = config.get("min_overhang_unannotated_canonical", 20) # STAR default: 12
min_overhang_unannotated_non_canonical = config.get("min_overhang_unannotated_non_canonical", 30) # STAR default: 30
# see discussion here:
# https://groups.google.com/forum/#!msg/rna-star/h9oh10UlvhI/BfSPGivUHmsJ
seedSearchStartLmax = int(config.get("seedSearchStartLmax", 50))
read_files_command = config.get("read_files_command", "zcat")
min_cells_per_intron = config.get('min_cells_per_intron', 30)


rule all:
    input:
        #expand('mapping_second_pass/{sample_id}/Aligned.sortedByCoord.out.bam', sample_id=sample_ids)
        #"second_STAR_index",
        expand("rmdup/{sample_id}/Aligned.rmdup.out.bam", sample_id=sample_ids),
        #expand('mapping_first_pass/{sample_id}/SJ.out.tab', sample_id=sample_ids)


rule make_first_STAR_index:
    output:
        directory("first_STAR_index")
    threads: workflow.cores
    shell:
        """
        mkdir -p {output} && STAR --runMode genomeGenerate --genomeDir {output} --genomeFastaFiles {config[fasta_path]} --sjdbGTFfile {config[gtf_path]} --sjdbOverhang {config[sjdb_overhang]} --runThreadN {threads}
        """

rule load_index_first_pass:
    input:
        "first_STAR_index"
    output:
        touch("first_index_loaded.flag")
    shell:
        "STAR --genomeDir {input} --genomeLoad LoadAndExit"


rule map_first_pass:
    input:
        lambda wildcards: fastq_paths.loc[wildcards.sample_id].fastq_1,
        lambda wildcards: fastq_paths.loc[wildcards.sample_id].fastq_2,
        "first_index_loaded.flag"
    output:
        'mapping_first_pass/{sample_id}/SJ.out.tab'
    priority: 100
    threads: 1
    shell:
        """
        STAR --genomeDir first_STAR_index --genomeLoad LoadAndKeep --readFilesIn {input[0]} {input[1]} --readFilesCommand {read_files_command} --outFileNamePrefix mapping_first_pass/{wildcards.sample_id}/ --runThreadN {threads} --outSJfilterReads Unique --outSAMtype None --seedSearchStartLmax {seedSearchStartLmax} --outSJfilterOverhangMin {min_overhang_unannotated_non_canonical} {min_overhang_unannotated_canonical} {min_overhang_unannotated_canonical} {min_overhang_unannotated_canonical} --alignEndsType EndToEnd
        """


rule unload_index_first_pass:
    input:
        expand('mapping_first_pass/{sample_id}/SJ.out.tab', sample_id=sample_ids)
    output:
        touch("first_index_unloaded.flag")
    shell:
        "STAR --genomeDir first_STAR_index --genomeLoad Remove"


rule filter_SJ:
    input:
        'mapping_first_pass/{sample_id}/SJ.out.tab',
        "first_index_unloaded.flag"
    output:
        'mapping_first_pass/{sample_id}/SJ.bed',
    shell:
        "awk '$6 == 0' {input[0]} | cut -f1-4 | sort > {output}"


rule merge_SJ_split_one:
    input:
        lambda wildcards: expand('mapping_first_pass/{sample_id}/SJ.bed', sample_id=sample_ids[int(wildcards.split1)::n_splits][int(wildcards.split2)::n_splits])
    output:
        temp("SJ.{split1}.{split2}.merged.bed")
    shell:
        "sort -m {input} > {output}"


rule merge_SJ_split_two:
    input:
        expand('SJ.{{split1}}.{split2}.merged.bed', split2=splits)
    output:
        temp("SJ.{split1}.merged.bed")
    shell:
        "sort -m {input} > {output}"


rule merge_SJ:
    input:
        expand('SJ.{split}.merged.bed', split=splits)
    output:
        "SJ.merged.bed"
    shell:
        "sort -m {input} | uniq -c | sed -r 's/^( *[^ ]+) +/\\1\t/' | awk '$1 >= {min_cells_per_intron}' | cut -f2-5 > {output}"


rule make_second_STAR_index:
    input:
        "SJ.merged.bed"
    output:
        directory("second_STAR_index")
    threads: workflow.cores
    shell:
        """
        mkdir -p {output} && STAR --runMode genomeGenerate --genomeDir {output} --genomeFastaFiles {config[fasta_path]} --sjdbGTFfile {config[gtf_path]} --sjdbOverhang {config[sjdb_overhang]} --runThreadN {threads} --sjdbFileChrStartEnd {input}
        """


rule load_index_second_pass:
    input:
        "second_STAR_index"
    output:
        touch("second_index_loaded.flag")
    shell:
        "STAR --genomeDir second_STAR_index --genomeLoad LoadAndExit"


rule map_second_pass:
    input:
        lambda wildcards: fastq_paths.loc[wildcards.sample_id].fastq_1,
        lambda wildcards: fastq_paths.loc[wildcards.sample_id].fastq_2,
        "second_index_loaded.flag"
    output:
        temp('mapping_second_pass/{sample_id}/Aligned.sortedByCoord.out.bam'),
    priority: 100
    threads: 1
    shell: """
    STAR --genomeDir second_STAR_index --genomeLoad LoadAndKeep --readFilesIn {input[0]} {input[1]} --readFilesCommand {read_files_command} --limitBAMsortRAM 4000000000 --outFileNamePrefix mapping_second_pass/{wildcards.sample_id}/ --runThreadN {threads} --outSJfilterReads Unique --outSAMtype BAM SortedByCoordinate --outSAMstrandField intronMotif --outFilterMultimapNmax 1 --outFilterType BySJout --seedSearchStartLmax {seedSearchStartLmax} --outSJfilterOverhangMin {min_overhang_unannotated_non_canonical} {min_overhang_unannotated_canonical} {min_overhang_unannotated_canonical} {min_overhang_unannotated_canonical} --alignEndsType EndToEnd
    """


rule unload_index_second_pass:
    input:
        expand('mapping_second_pass/{sample_id}/Aligned.sortedByCoord.out.bam', sample_id=sample_ids)
    output:
        touch("second_index_unloaded.flag")
    shell:
        "STAR --genomeDir second_STAR_index --genomeLoad Remove"


rule mark_duplicates_and_multimappers:
    input:
        "mapping_second_pass/{sample_id}/Aligned.sortedByCoord.out.bam",
        "second_index_unloaded.flag"
    output:
        temp("processed/{sample_id}/Processed.out.bam")
    #threads: workflow.cores
    shell:
        "STAR --runMode inputAlignmentsFromBAM --inputBAMfile {input[0]} --bamRemoveDuplicatesType UniqueIdentical --outFileNamePrefix processed/{wildcards.sample_id}/"


# -F 512 keeps only QC-pass
# -F 0x400 keeps only non-duplicate

rule filter_bam:
    input:
        "processed/{sample_id}/Processed.out.bam"
    output:
        protected("rmdup/{sample_id}/Aligned.rmdup.out.bam")
    shell:
        "samtools view -F 512 -u {input} | samtools view -F 0x400 -o {output}"
